{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "\n",
    "\n",
    " <h1>\n",
    "Welcome to the Math Question Answer Verification Competition! 泅\n",
    "\n",
    "The goal is to fine-tune a Llama-3-8B model to predict if a given solution to a math problem is correct or not. Your model should output True if the solution is correct, and False otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xStnwtpOqK0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl<0.17,>=0.14\n",
      "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.10.12-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.10.13-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting accelerate>=0.34.0 (from trl<0.17,>=0.14)\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=3.0.0 (from trl<0.17,>=0.14)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rich (from trl<0.17,>=0.14)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers>=4.46.0 (from trl<0.17,>=0.14)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m274.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wheel>=0.42.0 (from unsloth)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting packaging (from unsloth)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting numpy (from unsloth)\n",
      "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m286.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from unsloth)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m302.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting psutil (from unsloth)\n",
      "  Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth)\n",
      "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Downloading diffusers-0.35.2-py3-none-any.whl.metadata (20 kB)\n",
      "INFO: pip is looking at multiple versions of unsloth to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.10.11-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m247.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading unsloth-2025.10.10-py3-none-any.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m288.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=4.46.0 (from trl<0.17,>=0.14)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m269.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth\n",
      "  Downloading unsloth-2025.10.9-py3-none-any.whl.metadata (59 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m269.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading unsloth-2025.10.8-py3-none-any.whl.metadata (59 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m300.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchao>=0.13.0 (from unsloth_zoo)\n",
      "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "INFO: pip is looking at multiple versions of unsloth-zoo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.10.12-py3-none-any.whl.metadata (32 kB)\n",
      "  Downloading unsloth_zoo-2025.10.11-py3-none-any.whl.metadata (32 kB)\n",
      "  Downloading unsloth_zoo-2025.10.10-py3-none-any.whl.metadata (31 kB)\n",
      "  Downloading unsloth_zoo-2025.10.9-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pillow (from unsloth_zoo)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting regex (from unsloth_zoo)\n",
      "  Downloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m267.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msgspec (from unsloth_zoo)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting typing_extensions (from unsloth_zoo)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting filelock (from unsloth_zoo)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyyaml (from accelerate>=0.34.0->trl<0.17,>=0.14)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.0->trl<0.17,>=0.14)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m326.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting shellingham (from huggingface_hub>=0.34.0->unsloth)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub>=0.34.0->unsloth)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub>=0.34.0->unsloth)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting setuptools (from torch>=2.4.0->unsloth)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.46.0->trl<0.17,>=0.14)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting importlib_metadata (from diffusers->unsloth)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl<0.17,>=0.14)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->trl<0.17,>=0.14)\n",
      "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl<0.17,>=0.14)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.4.0->unsloth)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers->unsloth)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.4.0->unsloth)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m315.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets>=3.0.0->trl<0.17,>=0.14)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub>=0.34.0->unsloth)\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m211.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth-2025.10.8-py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m347.3/347.3 kB\u001b[0m \u001b[31m384.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.10.9-py3-none-any.whl (269 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m396.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m406.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m367.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m407.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m309.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m301.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m428.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m408.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m371.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m263.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m301.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m434.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m299.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m294.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m352.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m334.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m290.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m255.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m242.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m329.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m331.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m351.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m346.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m253.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m394.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m431.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m259.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m300.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m350.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m385.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m324.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m372.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading diffusers-0.35.2-py3-none-any.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m369.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m335.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m397.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m253.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m401.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m263.3/263.3 kB\u001b[0m \u001b[31m403.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m260.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m394.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m353.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m322.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m359.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m304.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m293.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m341.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m330.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m370.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m351.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m416.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m346.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m310.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m353.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m424.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m272.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m338.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m336.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m237.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m276.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m383.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m308.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m364.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m347.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m297.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m364.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m386.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m398.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m397.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m227.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m351.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m296.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m388.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m393.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m393.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m397.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao, pytz, nvidia-cusparselt-cu12, mpmath, zipp, xxhash, wheel, urllib3, tzdata, typing_extensions, tqdm, sympy, sniffio, six, shtab, setuptools, sentencepiece, safetensors, regex, pyyaml, pygments, pyarrow, psutil, protobuf, propcache, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgspec, mdurl, MarkupSafe, idna, hf-xet, hf_transfer, h11, fsspec, frozenlist, filelock, docstring-parser, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, typeguard, triton, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, jinja2, importlib_metadata, httpcore, anyio, aiosignal, rich, pandas, nvidia-cusolver-cu12, huggingface_hub, httpx, aiohttp, tyro, torch, tokenizers, diffusers, xformers, transformers, torchvision, datasets, cut_cross_entropy, bitsandbytes, accelerate, trl, peft, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.23.0\n",
      "    Uninstalling zipp-3.23.0:\n",
      "      Successfully uninstalled zipp-3.23.0\n",
      "  Attempting uninstall: xxhash\n",
      "    Found existing installation: xxhash 3.6.0\n",
      "    Uninstalling xxhash-3.6.0:\n",
      "      Successfully uninstalled xxhash-3.6.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.45.1\n",
      "    Uninstalling wheel-0.45.1:\n",
      "      Successfully uninstalled wheel-0.45.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.2.1\n",
      "    Uninstalling sentencepiece-0.2.1:\n",
      "      Successfully uninstalled sentencepiece-0.2.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.6.2\n",
      "    Uninstalling safetensors-0.6.2:\n",
      "      Successfully uninstalled safetensors-0.6.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.3\n",
      "    Uninstalling PyYAML-6.0.3:\n",
      "      Successfully uninstalled PyYAML-6.0.3\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.19.2\n",
      "    Uninstalling Pygments-2.19.2:\n",
      "      Successfully uninstalled Pygments-2.19.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.5\n",
      "    Uninstalling psutil-5.9.5:\n",
      "      Successfully uninstalled psutil-5.9.5\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: propcache\n",
      "    Found existing installation: propcache 0.4.1\n",
      "    Uninstalling propcache-0.4.1:\n",
      "      Successfully uninstalled propcache-0.4.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.3.0\n",
      "    Uninstalling pillow-11.3.0:\n",
      "      Successfully uninstalled pillow-11.3.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.5\n",
      "    Uninstalling networkx-3.5:\n",
      "      Successfully uninstalled networkx-3.5\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.7.0\n",
      "    Uninstalling multidict-6.7.0:\n",
      "      Successfully uninstalled multidict-6.7.0\n",
      "  Attempting uninstall: mdurl\n",
      "    Found existing installation: mdurl 0.1.2\n",
      "    Uninstalling mdurl-0.1.2:\n",
      "      Successfully uninstalled mdurl-0.1.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.3\n",
      "    Uninstalling MarkupSafe-3.0.3:\n",
      "      Successfully uninstalled MarkupSafe-3.0.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.2.0\n",
      "    Uninstalling hf-xet-1.2.0:\n",
      "      Successfully uninstalled hf-xet-1.2.0\n",
      "  Attempting uninstall: hf_transfer\n",
      "    Found existing installation: hf_transfer 0.1.9\n",
      "    Uninstalling hf_transfer-0.1.9:\n",
      "      Successfully uninstalled hf_transfer-0.1.9\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.8.0\n",
      "    Uninstalling frozenlist-1.8.0:\n",
      "      Successfully uninstalled frozenlist-1.8.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.20.0\n",
      "    Uninstalling filelock-3.20.0:\n",
      "      Successfully uninstalled filelock-3.20.0\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring_parser 0.17.0\n",
      "    Uninstalling docstring_parser-0.17.0:\n",
      "      Successfully uninstalled docstring_parser-0.17.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.4\n",
      "    Uninstalling charset-normalizer-3.4.4:\n",
      "      Successfully uninstalled charset-normalizer-3.4.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.10.5\n",
      "    Uninstalling certifi-2025.10.5:\n",
      "      Successfully uninstalled certifi-2025.10.5\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 25.4.0\n",
      "    Uninstalling attrs-25.4.0:\n",
      "      Successfully uninstalled attrs-25.4.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.6.1\n",
      "    Uninstalling aiohappyeyeballs-2.6.1:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.22.0\n",
      "    Uninstalling yarl-1.22.0:\n",
      "      Successfully uninstalled yarl-1.22.0\n",
      "  Attempting uninstall: typeguard\n",
      "    Found existing installation: typeguard 4.4.4\n",
      "    Uninstalling typeguard-4.4.4:\n",
      "      Successfully uninstalled typeguard-4.4.4\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 4.0.0\n",
      "    Uninstalling markdown-it-py-4.0.0:\n",
      "      Successfully uninstalled markdown-it-py-4.0.0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.6\n",
      "    Uninstalling Jinja2-3.1.6:\n",
      "      Successfully uninstalled Jinja2-3.1.6\n",
      "  Attempting uninstall: importlib_metadata\n",
      "    Found existing installation: importlib_metadata 8.7.0\n",
      "    Uninstalling importlib_metadata-8.7.0:\n",
      "      Successfully uninstalled importlib_metadata-8.7.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.11.0\n",
      "    Uninstalling anyio-4.11.0:\n",
      "      Successfully uninstalled anyio-4.11.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.4.0\n",
      "    Uninstalling aiosignal-1.4.0:\n",
      "      Successfully uninstalled aiosignal-1.4.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.9.4\n",
      "    Uninstalling rich-13.9.4:\n",
      "      Successfully uninstalled rich-13.9.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.13.1\n",
      "    Uninstalling aiohttp-3.13.1:\n",
      "      Successfully uninstalled aiohttp-3.13.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.35.2\n",
      "    Uninstalling diffusers-0.35.2:\n",
      "      Successfully uninstalled diffusers-0.35.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.11.0\n",
      "    Uninstalling accelerate-1.11.0:\n",
      "      Successfully uninstalled accelerate-1.11.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.0 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
      "bigframes 2.27.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gradio 5.49.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
      "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 bitsandbytes-0.48.2 certifi-2025.10.5 charset_normalizer-3.4.4 cut_cross_entropy-25.1.1 datasets-4.3.0 diffusers-0.35.2 dill-0.4.0 docstring-parser-0.17.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 h11-0.16.0 hf-xet-1.2.0 hf_transfer-0.1.9 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-0.36.0 idna-3.11 importlib_metadata-8.7.0 jinja2-3.1.6 markdown-it-py-4.0.0 mdurl-0.1.2 mpmath-1.3.0 msgspec-0.19.0 multidict-6.7.0 multiprocess-0.70.16 networkx-3.5 numpy-2.3.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 pandas-2.3.3 peft-0.17.1 pillow-12.0.0 propcache-0.4.1 protobuf-6.33.0 psutil-7.1.3 pyarrow-22.0.0 pygments-2.19.2 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.10.23 requests-2.32.5 rich-14.2.0 safetensors-0.6.2 sentencepiece-0.2.1 setuptools-80.9.0 shtab-1.7.2 six-1.17.0 sniffio-1.3.1 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 torchao-0.14.1 torchvision-0.23.0 tqdm-4.67.1 transformers-4.56.2 triton-3.4.0 trl-0.16.1 typeguard-4.4.4 typing_extensions-4.15.0 tyro-0.9.35 tzdata-2025.2 unsloth-2025.10.8 unsloth_zoo-2025.10.9 urllib3-2.5.0 wheel-0.45.1 xformers-0.0.32.post2 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "0fe2a6886dec4ad4b5ace8af9f239abb",
       "pip_warning": {
        "packages": [
         "PIL",
         "_distutils_hack",
         "certifi",
         "dateutil",
         "google",
         "numpy",
         "packaging",
         "psutil",
         "six"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall --no-cache-dir \"trl>=0.14,<0.17\" unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Load the Model and Tokenizer**\n",
    "\n",
    "Next, we'll load the Llama-3-8B model, which is the only model permitted for this competition. We'll use Unsloth's FastLanguageModel to handle this efficiently.\n",
    "\n",
    "A key technique we'll use is 4-bit quantization (load_in_4bit = True). Think of this as compressing the model's knowledge into a much smaller file size. This significantly reduces the amount of GPU memory required, allowing us to fine-tune this large model even on a free platform like Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URSw7qlhqlgB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "洶･ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "洶･ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35020d51c994a8ebb6bcea031313026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb748e06952645d4b2fec6c65c5cc3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0503ed4ed6f24716a9e8fc3d5cf19dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999d14d1048a4aaaad3b51d02f401df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e8bd16e18740d4a953d111e1b4cf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer for training.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 4096          # Context length (inputs + solution + answer + label)\n",
    "dtype = torch.bfloat16         # L4 supports bfloat16 natively (good perf / low memory)\n",
    "load_in_4bit = True            # 4-bit quantization to fit 8B on Colab\n",
    "\n",
    "# Load base Llama3-8B model + tokenizer\n",
    "# NOTE:\n",
    "# - We load the official Unsloth-wrapped Llama 3 8B Instruct weights.\n",
    "#   (\"Instruct\" generally gives better reasoning/QA behavior for this task)\n",
    "# - We are NOT starting from any pre-quantized LoRA checkpoint; this is the base model.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\", # Competition-approved model\n",
    "    max_seq_length  = max_seq_length,\n",
    "    dtype           = dtype,\n",
    "    load_in_4bit    = load_in_4bit,\n",
    ")\n",
    "\n",
    "# --- Put model in train mode (enables grad, LoRA attach later, etc.) ---\n",
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "print(\"Loaded model and tokenizer for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Prepare the Dataset**\n",
    "\n",
    "This is a crucial step where we format our data into a structure the model can learn from. The process involves three parts:\n",
    "\n",
    "1.  **Loading**: We'll load the official competition dataset from Hugging Face.\n",
    "2.  **Splitting**: The full dataset is massive. For this starter notebook, we'll create a much smaller, more manageable version to speed things up: **60,000 samples for training** and **5000 for validation**.\n",
    "3.  **Prompting**: We will format each data sample into a clear instructional prompt. This helps the model understand its role as a mathematician verifying a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ft0GynlvnC4n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer] eos_token='<|end_of_text|>' | pad_token='<|finetune_right_pad_id|>' | padding_side=right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7125014b9cce4d62ac089075ac13c436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa278f50dbcc4925bd5b5f0799fc237b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eb4d5d9db94381a234732280659599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfb2c8ef18842018e14b331899d3c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0584b779f2ae41f4abf915bd4b178c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9af74710fa43e1a346a057ff4224d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f0cd18579457eb3e4affc24ac167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting balanced train:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c03ec86db9340ac91e38deb81e6e3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting validation:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data summary ===\n",
      "Raw Train (pre-balance): 995000 samples | True=397500 False=597500 | True ratio=0.399\n",
      "Train Balanced (used for SFT): 60000 samples | True=30000 False=30000 | True ratio=0.500\n",
      "Validation (held-out): 5000 samples | True=2500 False=2500 | True ratio=0.500\n",
      "Formatted sizes 竊 train=60000 | val=5000\n",
      "Sample formatted example:\n",
      " You are a rigorous mathematician. Determine if the provided solution correctly solves the problem. Reply strictly with 'True' if it is fully correct, otherwise reply 'False'.\n",
      "\n",
      "Question:\n",
      "How many of the positive divisors of 3240 are multiples of 3?\n",
      "\n",
      "Solution:\n",
      "We can use sympy to get all divisors and count the ones that are multiples of 3.\n",
      "<llm-code>\n",
      "from sympy import divisors\n",
      "\n",
      "all_divisors = diviso\n"
     ]
    }
   ],
   "source": [
    "# Step 3 (Hardened): Data loading, stratified split, balance, formatting\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np, random\n",
    "import math\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "#  0) Safety: tokenizer must exist & have pad_token\n",
    "try:\n",
    "    _ = tokenizer.encode(\"ok\", add_special_tokens=False)\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"Tokenizer is not defined. Run the model/tokenizer cell first.\") from e\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.padding_side != \"right\":\n",
    "    tokenizer.padding_side = \"right\"  # better with causal LMs + LoRA SFT\n",
    "\n",
    "print(f\"[Tokenizer] eos_token={tokenizer.eos_token!r} | pad_token={tokenizer.pad_token!r} | padding_side={tokenizer.padding_side}\")\n",
    "\n",
    "#  1) Load ONLY the official train split\n",
    "# trust_remote_code=False to avoid executing custom code from HF repo\n",
    "full_train = load_dataset(\n",
    "    \"ad6398/nyu-dl-teach-maths-comp\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "#  2) STRATIFIED train/val split (deterministic 50/50 ratio for VAL)\n",
    "VAL_SIZE = 5000  # can tune if you want a larger validation\n",
    "y = np.array([bool(v) for v in full_train[\"is_correct\"]], dtype=bool)\n",
    "idx_all = np.arange(len(full_train))\n",
    "\n",
    "true_idx  = idx_all[y]\n",
    "false_idx = idx_all[~y]\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# we try to sample VAL_SIZE/2 positives and VAL_SIZE/2 negatives (or as close as possible)\n",
    "half_val = VAL_SIZE // 2\n",
    "val_true_sel  = rng.choice(true_idx,  size=min(half_val, len(true_idx)),  replace=False)\n",
    "val_false_sel = rng.choice(false_idx, size=min(half_val, len(false_idx)), replace=False)\n",
    "\n",
    "val_idx = np.concatenate([val_true_sel, val_false_sel])\n",
    "# if we are still short because one class was too small, top up from the other\n",
    "if len(val_idx) < VAL_SIZE:\n",
    "    needed = VAL_SIZE - len(val_idx)\n",
    "    # pick remaining from whichever class still has room\n",
    "    remain_pool = np.setdiff1d(idx_all, val_idx, assume_unique=False)\n",
    "    # deterministic top-up\n",
    "    extra_idx = rng.choice(remain_pool, size=needed, replace=False)\n",
    "    val_idx = np.concatenate([val_idx, extra_idx])\n",
    "\n",
    "# final dedupe + sort for reproducibility\n",
    "val_idx = np.unique(val_idx)\n",
    "train_idx = np.setdiff1d(idx_all, val_idx, assume_unique=False)\n",
    "\n",
    "raw_train = full_train.select(train_idx.tolist())\n",
    "raw_val   = full_train.select(val_idx.tolist())\n",
    "\n",
    "#  3) Balance TRAIN and cap per class for Colab budget\n",
    "ys_train = np.array([bool(v) for v in raw_train[\"is_correct\"]], dtype=bool)\n",
    "idx_true_train  = np.where(ys_train)[0].tolist()\n",
    "idx_false_train = np.where(~ys_train)[0].tolist()\n",
    "\n",
    "n_min = min(len(idx_true_train), len(idx_false_train))\n",
    "\n",
    "# You can raise CAP_PER_CLASS if VRAM/time allows (e.g. 30_000+ each for better accuracy).\n",
    "# L4 + LoRA 4-bit can usually handle ~40k-60k total steps if you tune batch size / grad_accum.\n",
    "CAP_PER_CLASS = min(30_000, n_min)\n",
    "\n",
    "idx_true_train  = idx_true_train[:CAP_PER_CLASS]\n",
    "idx_false_train = idx_false_train[:CAP_PER_CLASS]\n",
    "\n",
    "balanced_local_idx = idx_true_train + idx_false_train\n",
    "# Deterministic shuffle for reproducibility\n",
    "rng_perm = np.random.RandomState(SEED)\n",
    "rng_perm.shuffle(balanced_local_idx)\n",
    "\n",
    "train_balanced = raw_train.select(balanced_local_idx)\n",
    "\n",
    "#  4) Smart truncation for long solutions (keep head+tail)\n",
    "# keep more head reasoning to help correctness judgment but still stay under context\n",
    "HEAD_TOK, TAIL_TOK = 800, 256\n",
    "EOS = tokenizer.eos_token\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK):\n",
    "        return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "# 5) Concise instruction templates\n",
    "# NOTE: we REMOVE randomness in template choice.\n",
    "# Stochastic prompting during supervised fine-tuning can inject label noise.\n",
    "TEMPLATES = [\n",
    "    (\n",
    "        \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "        \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "        \"otherwise reply 'False'.\\n\\n\"\n",
    "        \"Question:\\n{q}\\n\\nSolution:\\n{s}\\n\\nAnswer:\\n{y}\"\n",
    "    )\n",
    "]\n",
    "\n",
    "def lbl(v):\n",
    "    return \"True\" if bool(v) else \"False\"\n",
    "\n",
    "def format_batch(batch):\n",
    "    qs, ss, ys = batch[\"question\"], batch[\"solution\"], batch[\"is_correct\"]\n",
    "    out = []\n",
    "    for q, s, yv in zip(qs, ss, ys):\n",
    "        q_str = \"\" if q is None else str(q)\n",
    "        s_str = smart_clip(\"\" if s is None else str(s))\n",
    "        tmpl = TEMPLATES[0]  # deterministic\n",
    "        out.append(tmpl.format(q=q_str, s=s_str, y=lbl(yv)) + EOS)\n",
    "    return {\"text\": out}\n",
    "\n",
    "# We keep originals intact; only drop columns in the formatted copies.\n",
    "formatted_train = train_balanced.map(\n",
    "    format_batch,\n",
    "    batched=True,\n",
    "    desc=\"Formatting balanced train\",\n",
    ").remove_columns([c for c in train_balanced.column_names if c != \"text\"])\n",
    "\n",
    "formatted_val = raw_val.map(\n",
    "    format_batch,\n",
    "    batched=True,\n",
    "    desc=\"Formatting validation\",\n",
    ").remove_columns([c for c in raw_val.column_names if c != \"text\"])\n",
    "\n",
    "#  6) Reports\n",
    "def class_report(ds, name):\n",
    "    if \"is_correct\" in ds.column_names:\n",
    "        yv = [bool(v) for v in ds[\"is_correct\"]]\n",
    "        t = sum(yv)\n",
    "        f = len(yv) - t\n",
    "        ratio = (t / len(yv)) if len(yv) else 0.0\n",
    "        print(f\"{name}: {len(yv)} samples | True={t} False={f} | True ratio={ratio:.3f}\")\n",
    "    else:\n",
    "        print(f\"{name}: no labels column\")\n",
    "\n",
    "print(\"=== Data summary ===\")\n",
    "class_report(raw_train, \"Raw Train (pre-balance)\")\n",
    "class_report(train_balanced, \"Train Balanced (used for SFT)\")\n",
    "class_report(raw_val, \"Validation (held-out)\")\n",
    "\n",
    "print(f\"Formatted sizes 竊 train={len(formatted_train)} | val={len(formatted_val)}\")\n",
    "\n",
    "print(\"Sample formatted example:\\n\", formatted_train[0][\"text\"][:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Fs1qmn37-F"
   },
   "source": [
    "## **Step 4: Configure LoRA and Set Up the Trainer**\n",
    "\n",
    "### **LoRA Configuration**\n",
    "\n",
    "Instead of training the entire model (which has billions of parameters), we'll use a technique called **Lo**w-**R**ank **A**daptation (LoRA). 沁幢ｸ十n",
    "\n",
    "Think of it like this: rather than rewriting an entire textbook, we're just adding small, efficient \"sticky notes\" (the LoRA adapters) to update the model's knowledge. This is much faster and requires significantly less memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEaRjozB3tz8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.10.8 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After fix] eos_token = <|end_of_text|> | pad_token = <|end_of_text|> | pad_id = 128001 | eos_id = 128001\n",
      "\"True\" -> [2575] | \"False\" -> [4139]\n",
      "LoRA configured (r=32, ﾎｱ=64, dropout=0.05) with gradient checkpointing.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Configure LoRA (High-capacity variant for accuracy)\n",
    "\n",
    "# Assumes `model` and `tokenizer` are already loaded and prepped for training.\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# --- LoRA configuration ---\n",
    "# Higher-capacity LoRA:\n",
    "# r=32 / lora_alpha=64 gives the adapter more expressive power.\n",
    "# We keep a small dropout to reduce overfitting on balanced 40k samples.\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,                              # LoRA rank (higher capacity)\n",
    "    lora_alpha = 64,                     # typically ~2 * r\n",
    "    lora_dropout = 0.05,                 # keep regularization to avoid overfit spikes\n",
    "    bias = \"none\",\n",
    "    target_modules = [\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",   # attention projections\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",     # MLP projections\n",
    "    ],\n",
    "    use_gradient_checkpointing = \"unsloth\",    # keeps memory in check\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "#  Padding / special tokens safety\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"[After fix] eos_token =\", tokenizer.eos_token,\n",
    "      \"| pad_token =\", tokenizer.pad_token,\n",
    "      \"| pad_id =\", tokenizer.pad_token_id,\n",
    "      \"| eos_id =\", tokenizer.eos_token_id)\n",
    "\n",
    "#  Label token sanity check\n",
    "true_ids  = tokenizer.encode(\"True\",  add_special_tokens=False)\n",
    "false_ids = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "\n",
    "print(f'\"True\" -> {true_ids} | \"False\" -> {false_ids}')\n",
    "\n",
    "if len(true_ids) != 1 or len(false_ids) != 1:\n",
    "    print(\"Warning: 'True' and/or 'False' are not single tokens. \"\n",
    "          \"We'll handle comparison by string prefix during evaluation.\")\n",
    "\n",
    "print(\"LoRA configured (r=32, ﾎｱ=64, dropout=0.05) with gradient checkpointing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvUgubHSpMcd"
   },
   "source": [
    "### **Second Fine Tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVX3rb9EpLMC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer] eos_token = <|end_of_text|> | pad_token = <|end_of_text|> | pad_id = 128001 | eos_id = 128001\n",
      "\"True\" -> [2575] | \"False\" -> [4139]\n",
      "Trainable parameters: 83,886,080 (LoRA should be ~1% of total)\n",
      "笨 Ready to continue fine-tuning using existing LoRA adapters.\n"
     ]
    }
   ],
   "source": [
    "# Step 4 (resume-safe): finalize training mode & sanity checks\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# You already ran FastLanguageModel.for_training(model) right after loading.\n",
    "# Do NOT call get_peft_model() again.\n",
    "\n",
    "# Pad/eos safety (keeps dataloader happy)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"[Tokenizer] eos_token =\", tokenizer.eos_token,\n",
    "      \"| pad_token =\", tokenizer.pad_token,\n",
    "      \"| pad_id =\", tokenizer.pad_token_id,\n",
    "      \"| eos_id =\", tokenizer.eos_token_id)\n",
    "\n",
    "# Label token sanity check (for Step 6/7 scoring path)\n",
    "true_ids  = tokenizer.encode(\"True\",  add_special_tokens=False)\n",
    "false_ids = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "print(f'\"True\" -> {true_ids} | \"False\" -> {false_ids}')\n",
    "if len(true_ids) != 1 or len(false_ids) != 1:\n",
    "    print(\"Note: 'True' and/or 'False' are multi-token 窶 scoring code handles this.\")\n",
    "\n",
    "# Optional: show trainable params to confirm LoRA is active\n",
    "try:\n",
    "    from peft import get_peft_model_state_dict\n",
    "    trainable = sum(p.numel() for n,p in model.named_parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable:,} (LoRA should be ~1% of total)\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"笨 Ready to continue fine-tuning using existing LoRA adapters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "### **SFTTrainer Setup**\n",
    "\n",
    "Now we'll set up the `SFTTrainer` (Supervised Fine-tuning Trainer). This is the main tool from the `trl` library that will handle the entire training loop for us. We'll give it our model, tokenizer, dataset, and a set of training instructions, such as the batch size and number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 5: Start Training\\!**\n",
    "\n",
    "Now, we'll call the `train()` function on our `trainer` object. This will kick off the fine-tuning process.\n",
    "\n",
    "Grab a coffee, as this will take a few hours\\! 笘表n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVZHQ4y74BCG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 supported: True\n",
      "steps/epoch=1177 | max_steps=1412 (竕1.2 epochs)\n",
      "Legacy TrainingArguments detected -> proceeding WITHOUT in-train evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 40,000 | Num Epochs = 2 | Total steps = 1,412\n",
      "O^O/ \\_/ \\    Batch size per device = 34 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (34 x 1 x 1) = 34\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1412' max='1412' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1412/1412 1:04:09, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.617500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.603500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.581900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.494400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved last checkpoint to outputs-last (no eval available).\n"
     ]
    }
   ],
   "source": [
    "# Step 4 (Version-safe): SFTTrainer configuration\n",
    "\n",
    "import math, torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "print(f\"bfloat16 supported: {BF16_OK}\")\n",
    "\n",
    "per_device_bs = 34\n",
    "grad_accum    = 1\n",
    "effective_bs  = per_device_bs * grad_accum\n",
    "\n",
    "approx_steps_per_epoch = math.ceil(len(formatted_train) / effective_bs)\n",
    "\n",
    "\n",
    "steps_per_epoch = math.ceil(40_000 / effective_bs)\n",
    "target_max_steps = 1412        # 1.2 full epochs\n",
    "\n",
    "print(f\"steps/epoch={steps_per_epoch} | max_steps={target_max_steps} (竕1.2 epochs)\")\n",
    "\n",
    "\n",
    "\n",
    "#  Common kwargs for both modern and legacy TrainingArguments\n",
    "common_kwargs = dict(\n",
    "    per_device_train_batch_size = per_device_bs,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    warmup_ratio                = 0.05,\n",
    "    max_steps                   = target_max_steps,\n",
    "    learning_rate               = 1.2e-4,          # safer LR for LoRA r=32\n",
    "    weight_decay                = 0.05,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    fp16                        = (not BF16_OK),\n",
    "    bf16                        = BF16_OK,\n",
    "    logging_steps               = 50,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = 200,\n",
    "    optim                       = \"adamw_torch_fused\",    # will use bitsandbytes if available\n",
    "    max_grad_norm               = 0.3,\n",
    "    seed                        = 42,\n",
    "    output_dir                  = \"outputs\",\n",
    "    report_to                   = \"none\",\n",
    "    remove_unused_columns       = False,\n",
    "    gradient_checkpointing      = True,        # fine with Unsloth\n",
    "    group_by_length             = True,        # fewer pads => more stable\n",
    "    dataloader_num_workers      = 2,\n",
    ")\n",
    "\n",
    "# Try modern args with evaluation; if not supported, fall back\n",
    "try:\n",
    "    train_args = TrainingArguments(\n",
    "        **common_kwargs,\n",
    "        evaluation_strategy      = \"steps\",\n",
    "        eval_steps               = 250,\n",
    "        load_best_model_at_end   = True,\n",
    "        metric_for_best_model    = \"eval_loss\",\n",
    "        greater_is_better        = False,\n",
    "    )\n",
    "    use_eval = True\n",
    "    print(\"Using TrainingArguments with evaluation_strategy='steps'.\")\n",
    "except TypeError:\n",
    "    train_args = TrainingArguments(**common_kwargs)\n",
    "    use_eval = False\n",
    "    print(\"Legacy TrainingArguments detected -> proceeding WITHOUT in-train evaluation.\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = formatted_train,\n",
    "    eval_dataset        = (formatted_val if use_eval else None),\n",
    "    dataset_text_field  = \"text\",\n",
    "    max_seq_length      = max_seq_length,\n",
    "    packing             = False,\n",
    "    args                = train_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# If we trained without eval (legacy fallback), at least save the last checkpoint.\n",
    "if not use_eval:\n",
    "    trainer.save_model(\"outputs-last\")\n",
    "    tokenizer.save_pretrained(\"outputs-last\")\n",
    "    print(\"Saved last checkpoint to outputs-last (no eval available).\")\n",
    "else:\n",
    "    # Trainer will have already restored best checkpoint by eval_loss\n",
    "    trainer.save_model(\"outputs-best\")\n",
    "    tokenizer.save_pretrained(\"outputs-best\")\n",
    "    print(\"笨 Training finished 窶 best checkpoint (by eval_loss) loaded and saved to outputs-best.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmwa6Ti2sCdI"
   },
   "source": [
    "## **Second Fine Tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXocLnRNsG2L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 supported: True\n",
      "steps/epoch=1765 | max_steps=2647 (竕1.5 epochs)\n",
      "Legacy TrainingArguments detected -> proceeding WITHOUT in-train evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 60,000 | Num Epochs = 2 | Total steps = 2,647\n",
      "O^O/ \\_/ \\    Batch size per device = 34 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (34 x 1 x 1) = 34\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2647' max='2647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2647/2647 2:02:17, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.368100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "Saved last checkpoint to outputs_scale60k_last (no eval available).\n"
     ]
    }
   ],
   "source": [
    "# Step 4 (Version-safe): SFTTrainer configuration\n",
    "\n",
    "import math, torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "print(f\"bfloat16 supported: {BF16_OK}\")\n",
    "\n",
    "per_device_bs = 34\n",
    "grad_accum    = 1\n",
    "effective_bs  = per_device_bs * grad_accum\n",
    "\n",
    "steps_per_epoch  = math.ceil(len(formatted_train) / effective_bs)\n",
    "target_max_steps = int(steps_per_epoch * 1.5)   # 竕 1.5 epochs\n",
    "\n",
    "print(f\"steps/epoch={steps_per_epoch} | max_steps={target_max_steps} (竕1.5 epochs)\")\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "common_kwargs = dict(\n",
    "    per_device_train_batch_size = per_device_bs,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    warmup_ratio                = 0.05,\n",
    "    max_steps                   = target_max_steps,\n",
    "    learning_rate               = 1.1e-4,\n",
    "    weight_decay                = 0.05,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    fp16                        = (not BF16_OK),\n",
    "    bf16                        = BF16_OK,\n",
    "    logging_steps               = 100,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = 200,\n",
    "    optim                       = \"adamw_torch_fused\",\n",
    "    max_grad_norm               = 0.3,\n",
    "    seed                        = 42,\n",
    "    output_dir                  = \"outputs_scale60k\",\n",
    "    report_to                   = \"none\",\n",
    "    remove_unused_columns       = False,\n",
    "    gradient_checkpointing      = True,\n",
    "    group_by_length             = True,\n",
    "    dataloader_num_workers      = 2,\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_args = TrainingArguments(\n",
    "        **common_kwargs,\n",
    "        evaluation_strategy      = \"steps\",\n",
    "        eval_steps               = 200,\n",
    "        load_best_model_at_end   = True,\n",
    "        metric_for_best_model    = \"eval_loss\",\n",
    "        greater_is_better        = False,\n",
    "    )\n",
    "    use_eval = True\n",
    "    print(\"Using TrainingArguments with evaluation_strategy='steps'.\")\n",
    "except TypeError:\n",
    "    train_args = TrainingArguments(**common_kwargs)\n",
    "    use_eval = False\n",
    "    print(\"Legacy TrainingArguments detected -> proceeding WITHOUT in-train evaluation.\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = formatted_train,\n",
    "    eval_dataset        = (formatted_val if use_eval else None),\n",
    "    dataset_text_field  = \"text\",\n",
    "    max_seq_length      = max_seq_length,\n",
    "    packing             = False,\n",
    "    args                = train_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if not use_eval:\n",
    "    trainer.save_model(\"outputs_scale60k_last\")\n",
    "    tokenizer.save_pretrained(\"outputs_scale60k_last\")\n",
    "    print(\"Saved last checkpoint to outputs_scale60k_last (no eval available).\")\n",
    "else:\n",
    "    trainer.save_model(\"outputs_scale60k_best\")\n",
    "    tokenizer.save_pretrained(\"outputs_scale60k_best\")\n",
    "    print(\"笨 Training finished 窶 best checkpoint saved to outputs_scale60k_best.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlrabkNBZvzE"
   },
   "source": [
    "### **Third Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08Sft-brZvfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16 supported: True\n",
      "steps/epoch=1765 | extra_steps=700 (~0.4 epochs)\n",
      "Legacy TrainingArguments detected 竊 proceeding WITHOUT in-train evaluation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e6fcd38c094d68b824eea2fa3368a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128001}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 60,000 | Num Epochs = 1 | Total steps = 700\n",
      "O^O/ \\_/ \\    Batch size per device = 34 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (34 x 1 x 1) = 34\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 32:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.368500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "笨 Continuation checkpoint saved to: outputs_scale60k_cont_last\n"
     ]
    }
   ],
   "source": [
    "# CONTINUE FT: ~0.4 epochs with lower LR\n",
    "import math, torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "print(f\"bfloat16 supported: {BF16_OK}\")\n",
    "\n",
    "per_device_bs = 34\n",
    "grad_accum    = 1\n",
    "effective_bs  = per_device_bs * grad_accum\n",
    "\n",
    "steps_per_epoch = math.ceil(60_000 / effective_bs)  # 竕 1765 for 60k with bs=34\n",
    "extra_steps     = 700                                # 竕 0.4 epochs\n",
    "print(f\"steps/epoch={steps_per_epoch} | extra_steps={extra_steps} (~0.4 epochs)\")\n",
    "\n",
    "OUT_DIR = \"outputs_scale60k_cont\"\n",
    "\n",
    "common_kwargs = dict(\n",
    "    per_device_train_batch_size = per_device_bs,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    warmup_steps                = 50,          # tiny warmup for stability\n",
    "    max_steps                   = extra_steps, # steps in THIS continuation\n",
    "    learning_rate               = 8e-5,        # lower LR for gentle refinement\n",
    "    weight_decay                = 0.05,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    fp16                        = (not BF16_OK),\n",
    "    bf16                        = BF16_OK,\n",
    "    logging_steps               = 100,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = 200,\n",
    "    optim                       = \"adamw_torch_fused\",\n",
    "    max_grad_norm               = 0.3,\n",
    "    seed                        = 42,\n",
    "    output_dir                  = OUT_DIR,\n",
    "    report_to                   = \"none\",\n",
    "    remove_unused_columns       = False,\n",
    "    gradient_checkpointing      = True,\n",
    "    group_by_length             = True,\n",
    "    dataloader_num_workers      = 2,\n",
    ")\n",
    "\n",
    "# Try with evaluation; if unavailable, it falls back automatically.\n",
    "try:\n",
    "    train_args = TrainingArguments(\n",
    "        **common_kwargs,\n",
    "        evaluation_strategy      = \"steps\",\n",
    "        eval_steps               = 200,\n",
    "        load_best_model_at_end   = True,\n",
    "        metric_for_best_model    = \"eval_loss\",\n",
    "        greater_is_better        = False,\n",
    "    )\n",
    "    use_eval = True\n",
    "    print(\"Using TrainingArguments with evaluation_strategy='steps'.\")\n",
    "except TypeError:\n",
    "    train_args = TrainingArguments(**common_kwargs)\n",
    "    use_eval = False\n",
    "    print(\"Legacy TrainingArguments detected 竊 proceeding WITHOUT in-train evaluation.\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,          # already loaded from /content/outputs_scale60k_last\n",
    "    tokenizer          = tokenizer,\n",
    "    train_dataset      = formatted_train,\n",
    "    eval_dataset       = (formatted_val if use_eval else None),\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length     = 4096,\n",
    "    packing            = False,\n",
    "    args               = train_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save continuation checkpoint\n",
    "if not use_eval:\n",
    "    save_dir = OUT_DIR + \"_last\"\n",
    "else:\n",
    "    save_dir = OUT_DIR + \"_best\"\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"笨 Continuation checkpoint saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1no-V7AnaGp1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
      "Eval device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring validation set: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 5000/5000 [13:36<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Validation size: 5000\n",
      "Best threshold: -0.375000\n",
      "Validation accuracy: 86.70%\n",
      "=======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  Step 6 re-eval for continuation checkpoint\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch, torch.nn.functional as F, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "NEW_CKPT = \"outputs_scale60k_cont_last\"   # change if trainer printed a different folder\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "reload_dtype = torch.bfloat16 if BF16_OK else torch.float16\n",
    "max_seq_length = 4096\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name        = NEW_CKPT,\n",
    "    max_seq_length    = max_seq_length,\n",
    "    dtype             = reload_dtype,\n",
    "    load_in_4bit      = True,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "# pad/eos safety\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "print(\"Eval device:\", device)\n",
    "\n",
    "#  reuse your existing Step-6 helpers (smart_clip, build_prompt)\n",
    "HEAD_TOK, TAIL_TOK = 800, 256\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    s = \"\" if text is None else str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK): return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "INFERENCE_TEMPLATE = (\n",
    "    \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "    \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "    \"otherwise reply 'False'.\\n\\n\"\n",
    "    \"Question:\\n{q}\\n\\n\"\n",
    "    \"Solution:\\n{s}\\n\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "\n",
    "def build_prompt(q, s):\n",
    "    return INFERENCE_TEMPLATE.format(q=(\"\" if q is None else str(q)),\n",
    "                                     s=smart_clip(\"\" if s is None else str(s)))\n",
    "\n",
    "t_id = tokenizer.encode(\"True\",  add_special_tokens=False)[0]\n",
    "f_id = tokenizer.encode(\"False\", add_special_tokens=False)[0]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_margin(prompt: str) -> float:\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n",
    "    out = model(**enc)\n",
    "    logprobs = F.log_softmax(out.logits[0, -1], dim=-1)\n",
    "    return (logprobs[t_id] - logprobs[f_id]).item()\n",
    "\n",
    "# raw_val must be in memory from Step 3 (rebuild it if you restarted runtime)\n",
    "margins, golds = [], []\n",
    "for ex in tqdm(raw_val, desc=\"Scoring validation set\"):\n",
    "    margins.append(score_margin(build_prompt(ex[\"question\"], ex[\"solution\"])))\n",
    "    golds.append(bool(ex[\"is_correct\"]))\n",
    "\n",
    "margins = np.array(margins, dtype=np.float32)\n",
    "golds   = np.array(golds,   dtype=bool)\n",
    "\n",
    "cands = np.quantile(margins, np.linspace(0.02, 0.98, 25))\n",
    "best_acc, best_th = -1.0, 0.0\n",
    "for th in cands:\n",
    "    acc = ( (margins >= th) == golds ).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc, best_th = acc, th\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(f\"Validation size: {len(golds)}\")\n",
    "print(f\"Best threshold: {best_th:.6f}\")\n",
    "print(f\"Validation accuracy: {best_acc*100:.2f}%\")\n",
    "print(\"=======================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 6: Inference and Evaluation**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that our model is trained, we need to test it on our validation set. We'll use a slightly different prompt for inference窶俳ne where we leave the `Output:` section blank for the model to complete.\n",
    "\n",
    "Let's test it on a single example from our validation set to see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agvQR_Ku5wWY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
      "Model reloaded on device: cuda:0\n",
      "Validation examples: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring validation set: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 5000/5000 [13:27<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Validation size: 5000\n",
      "Best threshold: -0.498047\n",
      "Validation accuracy: 86.72%\n",
      "Model 'True' rate after threshold: 50.68%\n",
      "Ground truth 'True' rate: 50.00%\n",
      "=======================================\n",
      "\n",
      "#### QUESTION ####\n",
      "A five-digit integer will be chosen at random from all possible positive five-digit integers. What is the probability that the number's units digit will be less than 5? Express your answer as a common fraction.\n",
      "\n",
      "#### SOLUTION ####\n",
      "The probability that the units digit will be less than $5$ is equal to the ratio of numbers whose units digit is less than $5$ to the total number of $5$-digit numbers. \n",
      "There are $9$ possible units digits less than $5$.\n",
      "There are $89999$ numbers less than $100000$ whose units digit is less than $5$.\n",
      "So the answer is $\\boxed{\\frac{89999}{99999}}$.\n",
      "\n",
      "#### SCORES ####\n",
      "log p('True')  = -2.578125\n",
      "log p('False') = -0.079102\n",
      "margin(True-False) = -2.499023\n",
      "threshold = -0.498047\n",
      "\n",
      "#### MODEL PREDICTION ####\n",
      "False\n",
      "\n",
      "#### CORRECT ANSWER ####\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Inference and Evaluation\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# 0) Reload the trained model from checkpoint\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "reload_dtype = torch.bfloat16 if BF16_OK else torch.float16\n",
    "max_seq_length = 4096\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name        = \"outputs_scale60k_last\",\n",
    "    max_seq_length    = max_seq_length,\n",
    "    dtype             = reload_dtype,\n",
    "    load_in_4bit      = True,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model reloaded on device: {device}\")\n",
    "\n",
    "# 1) Validation data\n",
    "validation_dataset = raw_val\n",
    "print(f\"Validation examples: {len(validation_dataset)}\")\n",
    "\n",
    "# 2) Prompt builder (matches training)\n",
    "HEAD_TOK, TAIL_TOK = 800, 256\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK):\n",
    "        return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "INFERENCE_TEMPLATE = (\n",
    "    \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "    \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "    \"otherwise reply 'False'.\\n\\n\"\n",
    "    \"Question:\\n{q}\\n\\n\"\n",
    "    \"Solution:\\n{s}\\n\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, solution: str) -> str:\n",
    "    q_str = \"\" if question is None else str(question)\n",
    "    s_str = smart_clip(\"\" if solution is None else str(solution))\n",
    "    return INFERENCE_TEMPLATE.format(q=q_str, s=s_str)\n",
    "\n",
    "# 3) Label tokens (robust to multi-token cases) + scorer\n",
    "true_tokens  = tokenizer.encode(\"True\",  add_special_tokens=False)\n",
    "false_tokens = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "single_token_labels = (len(true_tokens) == 1 and len(false_tokens) == 1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_example(prompt: str):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n",
    "\n",
    "    if single_token_labels:\n",
    "        out = model(**enc)\n",
    "        last_logits = out.logits[0, -1]\n",
    "        logprobs = F.log_softmax(last_logits, dim=-1)\n",
    "        logp_true  = logprobs[true_tokens[0]].item()\n",
    "        logp_false = logprobs[false_tokens[0]].item()\n",
    "        return (logp_true - logp_false), logp_true, logp_false\n",
    "    else:\n",
    "        def seq_logp(label_ids):\n",
    "            ids = enc[\"input_ids\"][0].tolist()\n",
    "            space = max_seq_length - len(ids)\n",
    "            label_ids = label_ids[:max(space, 0)]\n",
    "            if not label_ids:\n",
    "                return float(\"-inf\")\n",
    "            inp = torch.tensor([ids + label_ids[:-1]], device=device)\n",
    "            attn = torch.ones_like(inp)\n",
    "            out = model(input_ids=inp, attention_mask=attn)\n",
    "            logits = out.logits\n",
    "            lp = 0.0\n",
    "            start_pos = inp.shape[1] - (len(label_ids) - 1)\n",
    "            for t in range(len(label_ids)):\n",
    "                pos = start_pos + t - 1\n",
    "                token_id = label_ids[t]\n",
    "                lp += F.log_softmax(logits[0, pos], dim=-1)[token_id].item()\n",
    "            return lp\n",
    "\n",
    "        lp_t = seq_logp(true_tokens)\n",
    "        lp_f = seq_logp(false_tokens)\n",
    "        return (lp_t - lp_f), lp_t, lp_f\n",
    "\n",
    "# 4) Score validation, find threshold, compute accuracy\n",
    "margins, golds = [], []\n",
    "for ex in tqdm(validation_dataset, desc=\"Scoring validation set\"):\n",
    "    prompt = build_prompt(ex[\"question\"], ex[\"solution\"])\n",
    "    margin, _, _ = score_example(prompt)\n",
    "    margins.append(margin)\n",
    "    golds.append(bool(ex[\"is_correct\"]))\n",
    "\n",
    "margins = np.array(margins, dtype=np.float32)\n",
    "golds   = np.array(golds,   dtype=bool)\n",
    "\n",
    "candidate_thresholds = np.quantile(margins, np.linspace(0.02, 0.98, 25))\n",
    "best_acc, best_th = -1.0, 0.0\n",
    "for th in candidate_thresholds:\n",
    "    preds = margins >= th\n",
    "    acc = (preds == golds).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc, best_th = acc, th\n",
    "\n",
    "final_preds = margins >= best_th\n",
    "final_acc = (final_preds == golds).mean()\n",
    "true_rate_pred = final_preds.mean()\n",
    "true_rate_gold = golds.mean()\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(f\"Validation size: {len(golds)}\")\n",
    "print(f\"Best threshold: {best_th:.6f}\")\n",
    "print(f\"Validation accuracy: {final_acc*100:.2f}%\")\n",
    "print(f\"Model 'True' rate after threshold: {true_rate_pred*100:.2f}%\")\n",
    "print(f\"Ground truth 'True' rate: {true_rate_gold*100:.2f}%\")\n",
    "print(\"=======================================\")\n",
    "\n",
    "# 5) One qualitative example\n",
    "idx = 10\n",
    "ex = validation_dataset[idx]\n",
    "prompt = build_prompt(ex[\"question\"], ex[\"solution\"])\n",
    "margin, lp_t, lp_f = score_example(prompt)\n",
    "pred_bool = (margin >= best_th)\n",
    "\n",
    "print(\"\\n#### QUESTION ####\")\n",
    "print(ex[\"question\"])\n",
    "print(\"\\n#### SOLUTION ####\")\n",
    "print(ex[\"solution\"])\n",
    "print(\"\\n#### SCORES ####\")\n",
    "print(f\"log p('True')  = {lp_t:.6f}\")\n",
    "print(f\"log p('False') = {lp_f:.6f}\")\n",
    "print(f\"margin(True-False) = {margin:.6f}\")\n",
    "print(f\"threshold = {best_th:.6f}\")\n",
    "print(\"\\n#### MODEL PREDICTION ####\")\n",
    "print(pred_bool)\n",
    "print(\"\\n#### CORRECT ANSWER ####\")\n",
    "print(bool(ex[\"is_correct\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 7: Generate Submission File**\n",
    "\n",
    "This is the final step\\! We will now run our fine-tuned model on the official `test` dataset.\n",
    "\n",
    "We will loop through each example in the test set, generate a prediction, and format the results into a CSV file with two columns: `ID` and `is_correct`, as required by the competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lvcDSh0JZYm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference device: cuda:0\n",
      "Test examples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring test: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 10000/10000 [26:46<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv saved.\n",
      "   ID  is_correct\n",
      "0   0        True\n",
      "1   1       False\n",
      "2   2       False\n",
      "3   3        True\n",
      "4   4       False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Generate Submission File (Test Inference 竊 CSV)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Eval mode / device\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Inference device: {device}\")\n",
    "\n",
    "# Inference template (must match Step 6)\n",
    "INFERENCE_TEMPLATE = (\n",
    "    \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "    \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "    \"otherwise reply 'False'.\\n\\n\"\n",
    "    \"Question:\\n{q}\\n\\n\"\n",
    "    \"Solution:\\n{s}\\n\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "\n",
    "# Same clipping + sequence length as training/inference\n",
    "HEAD_TOK, TAIL_TOK = 800, 256\n",
    "max_seq_length = 4096  # must match Step 6\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    s = \"\" if text is None else str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK):\n",
    "        return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "def build_prompt(question: str, solution: str) -> str:\n",
    "    return INFERENCE_TEMPLATE.format(\n",
    "        q=\"\" if question is None else str(question),\n",
    "        s=smart_clip(\"\" if solution is None else str(solution)),\n",
    "    )\n",
    "\n",
    "# Label tokenization (single- or multi-token safe)\n",
    "true_tokens  = tokenizer.encode(\"True\",  add_special_tokens=False)\n",
    "false_tokens = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "single = (len(true_tokens) == 1 and len(false_tokens) == 1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_margin(prompt: str) -> float:\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n",
    "    if single:\n",
    "        logits = model(**enc).logits[0, -1]\n",
    "        lp = F.log_softmax(logits, dim=-1)\n",
    "        return (lp[true_tokens[0]] - lp[false_tokens[0]]).item()\n",
    "    else:\n",
    "        def seq_logp(label_ids):\n",
    "            ids = enc[\"input_ids\"][0].tolist()\n",
    "            space = max_seq_length - len(ids)\n",
    "            label_ids = label_ids[:max(space, 0)]\n",
    "            if not label_ids:\n",
    "                return float(\"-inf\")\n",
    "            inp = torch.tensor([ids + label_ids[:-1]], device=device)\n",
    "            attn = torch.ones_like(inp)\n",
    "            logits = model(input_ids=inp, attention_mask=attn).logits\n",
    "            lp = 0.0\n",
    "            start = inp.shape[1] - (len(label_ids) - 1)\n",
    "            for t, tok in enumerate(label_ids):\n",
    "                lp += F.log_softmax(logits[0, start + t - 1], dim=-1)[tok].item()\n",
    "            return lp\n",
    "        return seq_logp(true_tokens) - seq_logp(false_tokens)\n",
    "\n",
    "# 1) Load official test split\n",
    "test_dataset = load_dataset(\n",
    "    \"ad6398/nyu-dl-teach-maths-comp\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "print(f\"Test examples: {len(test_dataset)}\")\n",
    "\n",
    "# 2) Predict with tuned threshold from Step 6 (best_th must be defined)\n",
    "predictions = []\n",
    "for ex in tqdm(test_dataset, desc=\"Scoring test\"):\n",
    "    prompt = build_prompt(ex[\"question\"], ex[\"solution\"])\n",
    "    margin = score_margin(prompt)\n",
    "    predictions.append(margin >= best_th)\n",
    "\n",
    "# 3) Build submission\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": np.arange(len(predictions)),\n",
    "    \"is_correct\": predictions,\n",
    "})\n",
    "\n",
    "# 4) Save\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved.\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5489c0ed"
   },
   "source": [
    "# SAVE THE MODEL TO DRIVE AND RUN INFERENCE\n",
    "Add code to save the model checkpoint to Google Drive, load the model from the checkpoint, and generate the final submission CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b1e0a43"
   },
   "source": [
    "## Mount google drive\n",
    "\n",
    "### Subtask:\n",
    "Mount Google Drive to save the model checkpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b8ab404"
   },
   "source": [
    "**Reasoning**:\n",
    "Mount Google Drive to save the model checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e020e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c28a7dd"
   },
   "source": [
    "## Save model checkpoint\n",
    "\n",
    "### Subtask:\n",
    "Save the trained model checkpoint to the specified path in Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe96ff59"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the save path and save the model and tokenizer to Google Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ec9d6bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint and tokenizer saved to: /content/drive/MyDrive/llama3_8b_math_verifier_checkpoint\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to save the model checkpoint in Google Drive\n",
    "save_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model checkpoint and tokenizer saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feb0b9a5"
   },
   "source": [
    "## Load model from checkpoint\n",
    "\n",
    "### Subtask:\n",
    "Load the model from the saved checkpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d984f7ec"
   },
   "source": [
    "**Reasoning**:\n",
    "Load the model and tokenizer from the saved checkpoint path in Google Drive and prepare the model for inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc269188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38110dda49d4448c8fbf2865ad887a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.8 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "笨 Model and tokenizer successfully loaded from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 4096\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype          = dtype,\n",
    "    load_in_4bit   = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "print(\"笨 Model and tokenizer successfully loaded from checkpoint.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
