{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "\n",
    "\n",
    " <h1>\n",
    "The Math Question Answer Verification Competition! ðŸš€\n",
    "\n",
    "The goal is to fine-tune a Llama-3-8B model to predict if a given solution to a math problem is correct or not. Your model should output True if the solution is correct, and False otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xStnwtpOqK0e",
    "outputId": "8a2beda3-2bef-46db-8521-5fdbacf7b6f3"
   },
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Load the Model and Tokenizer**\n",
    "\n",
    "Next, we'll load the Llama-3-8B model, which is the only model permitted for this competition. We'll use Unsloth's FastLanguageModel to handle this efficiently.\n",
    "\n",
    "A key technique we'll use is 4-bit quantization (load_in_4bit = True). Think of this as compressing the model's knowledge into a much smaller file size. This significantly reduces the amount of GPU memory required, allowing us to fine-tune this large model even on a free platform like Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376,
     "referenced_widgets": [
      "c0d02d53a450476d86ce6ad0dac97243",
      "71ad39754590400fb61da739dd5c4bd2",
      "6acb506a36df41699b7407fd710cca9d",
      "02918583f55c425f92b8bfbfa6f47cd1",
      "de31d690e4314d44857203a03e84b009",
      "183c2eb793fd4eeb98bb78c95bf937d5",
      "ee7d30ec8d3948c49712b8690f701dcc",
      "c676e9001f754b64ab1a7505f85b033f",
      "53af7f5503074ec6a8163a6ea5473129",
      "ac550dc8250f40ea82f914c3d57fdd22",
      "4f46a45d0b9a414c80890b01c69f8ac2",
      "b67b7fb2173f45bb8b8fe0da16d4f440",
      "b4583dd9871a4231b1cdf23b63f9cad3",
      "56468d88c6684ece9b8f7a8d631acd47",
      "bee59b91bbb6484c866eb49261ede44e",
      "15a4518142e1463985318b72f04f5961",
      "9ce64541b94942d79007e2b33513fbc1",
      "b380b812b6f14577b0f3feea8228726d",
      "bc3cc09007ef4b108e2d73742620a658",
      "718b7378b46740b3bea99877e122443d",
      "01b31b3ede1c46a6acbdfea9f802f5be",
      "1f895be75f874ab2a03fe9230ec4537f",
      "45a55877ea7b4eefa2c6b0e62327ae79",
      "a11a164d5dfa477b912c573361353c2d",
      "d955e218f5d74a03888f5b7810569629",
      "26535fd2de5849cea411c477ba2f52d9",
      "05dfa77878ed458291ef94fa7fc3d279",
      "171593b151014aa9aa131e3ec8775c68",
      "42e69f2ad3714e5cb9fcc6b3b7987b56",
      "f37925d2f74e40bc9042afd9ed514253",
      "5f3a73d6082c44908453f1244f808b72",
      "cd3cfb5e0b6d45c9a8e251cec7a305f8",
      "f6dabeffef0948a88bb39c0219d403c0",
      "b6ed7ce299f048e19d5f6a824c0362b5",
      "d04fe30dbc884b60a991a25b90ad9e93",
      "b2761f7afefb4696aa6712ab0edb81a9",
      "77d520f6aa594f85aec44363dbb6e976",
      "479eb3f67d334c91a89ca644625c2f66",
      "fb30d82705c54f688fcbc8b719b3f051",
      "f9e9fd4637e44330994142ed4bad5381",
      "f7db05dbbda547c28a52078479610534",
      "7aa46606c7c1405d9dceda8c5b481ab7",
      "12811ac29673497c8a2fbdcbbb0fde49",
      "bdfce6be5b624e1e8abd38205b13b8da",
      "1ca53b975539498aa7fc2bce020a01a9",
      "8ca531f9864b4cb787ac3fc11dec018c",
      "4a0d4217b4d54c7badc31e121a08c3f1",
      "eae913ed712840768658c18ed1960f63",
      "c1fc2ea8515344a184de1dbfc2494c81",
      "9019d95938434bde9fddc6764e8833c9",
      "7513c07c4a7f4ef0b84f790040e5b44f",
      "e4ec913bcb47482a80595de4f6317e79",
      "2769768a6a1e4d5da65c041acce1b8af",
      "65ad1ee9251d434895ab55052f3b6a0f",
      "d9e80d0ad3584664bd08ba9d7634445e"
     ]
    },
    "id": "URSw7qlhqlgB",
    "outputId": "1f99826a-7722-4c0d-d0cd-6d8bd1f645b0"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "#  Configuration \n",
    "max_seq_length = 2048          # Context length (inputs + solution + answer + label)\n",
    "dtype = torch.bfloat16         # L4 supports bfloat16 natively (good perf / low memory)\n",
    "load_in_4bit = True            # 4-bit quantization to fit 8B on Colab\n",
    "\n",
    "#  Load base Llama3-8B model + tokenizer \n",
    "# NOTE:\n",
    "# - We load the official Unsloth-wrapped Llama 3 8B Instruct weights.\n",
    "#   (\"Instruct\" generally gives better reasoning/QA behavior for this task)\n",
    "# - We are NOT starting from any pre-quantized LoRA checkpoint; this is the base model.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\", # Competition-approved model\n",
    "    max_seq_length  = max_seq_length,\n",
    "    dtype           = dtype,\n",
    "    load_in_4bit    = load_in_4bit,\n",
    ")\n",
    "\n",
    "#  Put model in train mode (enables grad, LoRA attach later, etc.) \n",
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "print(\"Loaded model and tokenizer for training on L4 (bf16, 4-bit).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Prepare the Dataset**\n",
    "\n",
    "This is a crucial step where we format our data into a structure the model can learn from. The process involves three parts:\n",
    "\n",
    "1.  **Loading**: We'll load the official competition dataset from Hugging Face.\n",
    "2.  **Splitting**: The full dataset is massive. For this notebook, we'll create a much smaller, more manageable version to speed things up: **60,000 samples for training** and **5000 for validation**.\n",
    "3.  **Prompting**: We will format each data sample into a clear instructional prompt. This helps the model understand its role as a mathematician verifying a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614,
     "referenced_widgets": [
      "79215fdbc593448aaff0735e08816c00",
      "d58a772a6c5b4f9e9ae1bb70182a7065",
      "cdf04d0b857a408185c691d85bcc0e4c",
      "1cd201e833dd45b2abd66677b555f6b4",
      "5af8db4127354601809b6358ad74250d",
      "c4e62fbccb92449887ab4d443d9a5f6b",
      "f15db68a3bba446fa907c0401c77a050",
      "56d6a552461940bb970f837550617190",
      "6b18c72dfbc44f58a8b89bd7e74aacd5",
      "5398e83cc0fd42adbb064775acf2dc3d",
      "9c253a7c0ce84647aeaa85268a556f78",
      "a4d96ee02c194302856371ef6674770d",
      "08c6e86ede35454fa3961c428b4f1707",
      "f7e4ceb777d845b4ab5545ac441634a7",
      "a6367625a41848938be412d332a30de0",
      "3d31625730544f1aaa56f3a60a40ff4e",
      "6ba5259a63c24c088a6a600835cc9c3e",
      "4fe9b74a6c6c46ab890147c2436d0af5",
      "e40ec9f4757c4650a4801ff35c324240",
      "4056414ab98143d9bbb270e3de33096b",
      "1fa6bc2b882843a293b132de81983187",
      "7680b423dacf4f0f8077ab7b5e573b18",
      "78f6cce66dbe48679328d60dadf6663e",
      "0a8698b12132481cb01d49a7a4bb5b9d",
      "005210d601304592b552c236dc701ba7",
      "753a1df30544433a9afd0a34dc7854fe",
      "d3c4f41a253c403690efedb03999531f",
      "57be82a5d0d54ec881584d07cfc2bbd3",
      "3d5ecd88495d4d5181f6a1e78a8a3570",
      "b57eed606b5c4297b94a900fd336c731",
      "dc404f18db09470683eba2b0fad4dc04",
      "c055b985442b4200974f6677839ca5b9",
      "5502c41abf334bb88f30fde02d98fdc8",
      "872e0721d2b64c9b96b3f3c565baac11",
      "d7713a02a4bd4fd4bbcedde7ab91fd47",
      "faf963b76cf04f3882ea649906c2c989",
      "1b5e345f47714ac1a433737cccfba6f9",
      "787fd76050a049c18ea04e230b6670d9",
      "b970d560e3e5492d9ba3a976d1ec057f",
      "e6edb88637c744a996f04ac87613ecb3",
      "276c58ca54ca49a99203fa2e52309390",
      "b9a0fe6071b54be18497b43f7af0f663",
      "b42463c460154939b664db9f4a47336c",
      "0475c74360004d61862658fe881da8ab",
      "ccc4bf3d06c14239830075c950c08b4e",
      "5fdca5d6b9c24991ba5f8af367d0341a",
      "199963cc49d74a1a8a9b970d8f2014f4",
      "02cf1bd505d746118ba7f97f018e4425",
      "baf64453a5054f84b14106674d085229",
      "1220620c252f4269953134a575d1f077",
      "a4ffcc69e7724b05a6dcf66ff6c0a36d",
      "d3533a7b7e7e4c0ebc8fc319a5785847",
      "96cdc82dce614c3291e8eb98a2b0e239",
      "25ab1051a3a14bf8aea2fc8a7d2d4115",
      "db296d8cdd8041938f68c4bd25abb281",
      "f13660a431c04b4f9dcb826fc791745b",
      "60f1f086290e477e84b5cd9ade10930e",
      "9478b83ded694c949006fb88b898f9cf",
      "1c79ee7e844e4113bcaa7b17b37783ea",
      "fd5b4a8efa474b2781a058018e369fdb",
      "e666ffd6a97a46119a2427db851c74a5",
      "6e45c9ac3e3c452dac14f3ad0ee43a7b",
      "f7fb56b1cb4b432ba881f0023d11d9f3",
      "9705768be0554aa8936c3dc305c16de5",
      "1e4f081590e2424fab973973e1c71633",
      "d21a28f2f9ea45d882c4beca1ba51ae7",
      "2b914472a1574b66a81b011a4a5cb036",
      "067ced805c1b4efeb1a227d6a0c49683",
      "08535b29be364ebe977c17cd5a290c51",
      "47e58db6f79b45df9433cbafa1474cb6",
      "8346dc37f122416b85cbabcf40007e38",
      "506a8c3635514a56b08453028299a411",
      "f0f837c8f0a74d1190f41ee229ef2ac7",
      "6367081433dc4757a05a4ec07ee69fe9",
      "e039095c104f430eafcf8c09de0cff4b",
      "c7fe11639e8c498eb45a0c06efebedb8",
      "ecca5410323147749cb0db9ba8bb5faf",
      "461c839b7dc344af83cd65211837aea8",
      "e67466ab65b1439aa036507af1d9b6f5",
      "9c42e905bd064e90b9bc15da0b1365c6",
      "d4f0f08e1e92486ebcc300dffb6dd6ed",
      "7467542b9ec54c6e989656f03f801ae2",
      "57abde41d6d7490192209911d71990ac",
      "3930a1aa4c94426a95b1ddb0f13b6d8c",
      "3cc4296bf7784db5a1d61ac09870d553",
      "6e6c135ca17042f0b0e86e0a1bf4f90e",
      "453a1dcd132242eeaafb1f64889e3be7",
      "5ea85b1795184d23aeaadb15362a1c5a"
     ]
    },
    "id": "ft0GynlvnC4n",
    "outputId": "fa892bf1-65a4-4a6a-d991-4b3671612c2c"
   },
   "outputs": [],
   "source": [
    "# Step 3 (Hardened): Data loading, stratified split, balance, formatting\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np, random\n",
    "import math\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "#  0) Safety: tokenizer must exist & have pad_token\n",
    "try:\n",
    "    _ = tokenizer.encode(\"ok\", add_special_tokens=False)\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"Tokenizer is not defined. Run the model/tokenizer cell first.\") from e\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.padding_side != \"right\":\n",
    "    tokenizer.padding_side = \"right\"  # better with causal LMs + LoRA SFT\n",
    "\n",
    "print(f\"[Tokenizer] eos_token={tokenizer.eos_token!r} | pad_token={tokenizer.pad_token!r} | padding_side={tokenizer.padding_side}\")\n",
    "\n",
    "#  1) Load ONLY the official train split\n",
    "# trust_remote_code=False to avoid executing custom code from HF repo\n",
    "full_train = load_dataset(\n",
    "    \"ad6398/nyu-dl-teach-maths-comp\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "#  2) STRATIFIED train/val split (deterministic 50/50 ratio for VAL)\n",
    "VAL_SIZE = 5000  # can tune if you want a larger validation\n",
    "y = np.array([bool(v) for v in full_train[\"is_correct\"]], dtype=bool)\n",
    "idx_all = np.arange(len(full_train))\n",
    "\n",
    "true_idx  = idx_all[y]\n",
    "false_idx = idx_all[~y]\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# we try to sample VAL_SIZE/2 positives and VAL_SIZE/2 negatives (or as close as possible)\n",
    "half_val = VAL_SIZE // 2\n",
    "val_true_sel  = rng.choice(true_idx,  size=min(half_val, len(true_idx)),  replace=False)\n",
    "val_false_sel = rng.choice(false_idx, size=min(half_val, len(false_idx)), replace=False)\n",
    "\n",
    "val_idx = np.concatenate([val_true_sel, val_false_sel])\n",
    "# if we are still short because one class was too small, top up from the other\n",
    "if len(val_idx) < VAL_SIZE:\n",
    "    needed = VAL_SIZE - len(val_idx)\n",
    "    # pick remaining from whichever class still has room\n",
    "    remain_pool = np.setdiff1d(idx_all, val_idx, assume_unique=False)\n",
    "    # deterministic top-up\n",
    "    extra_idx = rng.choice(remain_pool, size=needed, replace=False)\n",
    "    val_idx = np.concatenate([val_idx, extra_idx])\n",
    "\n",
    "# final dedupe + sort for reproducibility\n",
    "val_idx = np.unique(val_idx)\n",
    "train_idx = np.setdiff1d(idx_all, val_idx, assume_unique=False)\n",
    "\n",
    "raw_train = full_train.select(train_idx.tolist())\n",
    "raw_val   = full_train.select(val_idx.tolist())\n",
    "\n",
    "#  3) Balance TRAIN and cap per class for Colab budget\n",
    "ys_train = np.array([bool(v) for v in raw_train[\"is_correct\"]], dtype=bool)\n",
    "idx_true_train  = np.where(ys_train)[0].tolist()\n",
    "idx_false_train = np.where(~ys_train)[0].tolist()\n",
    "\n",
    "n_min = min(len(idx_true_train), len(idx_false_train))\n",
    "\n",
    "# You can raise CAP_PER_CLASS if VRAM/time allows (e.g. 30_000+ each for better accuracy).\n",
    "# L4 + LoRA 4-bit can usually handle ~40k-60k total steps if you tune batch size / grad_accum.\n",
    "CAP_PER_CLASS = min(30_000, n_min)\n",
    "\n",
    "idx_true_train  = idx_true_train[:CAP_PER_CLASS]\n",
    "idx_false_train = idx_false_train[:CAP_PER_CLASS]\n",
    "\n",
    "balanced_local_idx = idx_true_train + idx_false_train\n",
    "# Deterministic shuffle for reproducibility\n",
    "rng_perm = np.random.RandomState(SEED)\n",
    "rng_perm.shuffle(balanced_local_idx)\n",
    "\n",
    "train_balanced = raw_train.select(balanced_local_idx)\n",
    "\n",
    "#  4) Smart truncation for long solutions (keep head+tail)\n",
    "# keep more head reasoning to help correctness judgment but still stay under context\n",
    "HEAD_TOK, TAIL_TOK = 800, 256\n",
    "EOS = tokenizer.eos_token\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK):\n",
    "        return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "#  5) Concise instruction templates\n",
    "# NOTE: we REMOVE randomness in template choice.\n",
    "# Stochastic prompting during supervised fine-tuning can inject label noise.\n",
    "TEMPLATES = [\n",
    "    (\n",
    "        \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "        \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "        \"otherwise reply 'False'.\\n\\n\"\n",
    "        \"Question:\\n{q}\\n\\nSolution:\\n{s}\\n\\nAnswer:\\n{y}\"\n",
    "    )\n",
    "]\n",
    "\n",
    "def lbl(v):\n",
    "    return \"True\" if bool(v) else \"False\"\n",
    "\n",
    "def format_batch(batch):\n",
    "    qs, ss, ys = batch[\"question\"], batch[\"solution\"], batch[\"is_correct\"]\n",
    "    out = []\n",
    "    for q, s, yv in zip(qs, ss, ys):\n",
    "        q_str = \"\" if q is None else str(q)\n",
    "        s_str = smart_clip(\"\" if s is None else str(s))\n",
    "        tmpl = TEMPLATES[0]  # deterministic\n",
    "        out.append(tmpl.format(q=q_str, s=s_str, y=lbl(yv)) + EOS)\n",
    "    return {\"text\": out}\n",
    "\n",
    "# We keep originals intact; only drop columns in the formatted copies.\n",
    "formatted_train = train_balanced.map(\n",
    "    format_batch,\n",
    "    batched=True,\n",
    "    desc=\"Formatting balanced train\",\n",
    ").remove_columns([c for c in train_balanced.column_names if c != \"text\"])\n",
    "\n",
    "formatted_val = raw_val.map(\n",
    "    format_batch,\n",
    "    batched=True,\n",
    "    desc=\"Formatting validation\",\n",
    ").remove_columns([c for c in raw_val.column_names if c != \"text\"])\n",
    "\n",
    "#  6) Reports\n",
    "def class_report(ds, name):\n",
    "    if \"is_correct\" in ds.column_names:\n",
    "        yv = [bool(v) for v in ds[\"is_correct\"]]\n",
    "        t = sum(yv)\n",
    "        f = len(yv) - t\n",
    "        ratio = (t / len(yv)) if len(yv) else 0.0\n",
    "        print(f\"{name}: {len(yv)} samples | True={t} False={f} | True ratio={ratio:.3f}\")\n",
    "    else:\n",
    "        print(f\"{name}: no labels column\")\n",
    "\n",
    "print(\"=== Data summary ===\")\n",
    "class_report(raw_train, \"Raw Train (pre-balance)\")\n",
    "class_report(train_balanced, \"Train Balanced (used for SFT)\")\n",
    "class_report(raw_val, \"Validation (held-out)\")\n",
    "\n",
    "print(f\"Formatted sizes â†’ train={len(formatted_train)} | val={len(formatted_val)}\")\n",
    "\n",
    "print(\"Sample formatted example:\\n\", formatted_train[0][\"text\"][:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Fs1qmn37-F"
   },
   "source": [
    "## **Step 4: Configure LoRA and Set Up the Trainer**\n",
    "\n",
    "### **LoRA Configuration**\n",
    "\n",
    "Instead of training the entire model (which has billions of parameters), we'll use a technique called **Lo**w-**R**ank **A**daptation (LoRA). \n",
    "\n",
    "Think of it like this: rather than rewriting an entire textbook, we're just adding small, efficient \"sticky notes\" (the LoRA adapters) to update the model's knowledge. This is much faster and requires significantly less memory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEaRjozB3tz8",
    "outputId": "9aab09ac-7ee1-49ae-e059-bf82296b3fb0"
   },
   "outputs": [],
   "source": [
    "# Step 4: Configure LoRA (High-capacity variant for accuracy)\n",
    "\n",
    "# Assumes `model` and `tokenizer` are already loaded and prepped for training.\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "#  LoRA configuration \n",
    "# Higher-capacity LoRA:\n",
    "# r=32 / lora_alpha=64 gives the adapter more expressive power.\n",
    "# We keep a small dropout to reduce overfitting on balanced 40k samples.\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,                              # LoRA rank (higher capacity)\n",
    "    lora_alpha = 64,                     # typically ~2 * r\n",
    "    lora_dropout = 0.05,                 # keep regularization to avoid overfit spikes\n",
    "    bias = \"none\",\n",
    "    target_modules = [\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",   # attention projections\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",     # MLP projections\n",
    "    ],\n",
    "    use_gradient_checkpointing = \"unsloth\",    # keeps memory in check\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "#  Padding / special tokens safety\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"[After fix] eos_token =\", tokenizer.eos_token,\n",
    "      \"| pad_token =\", tokenizer.pad_token,\n",
    "      \"| pad_id =\", tokenizer.pad_token_id,\n",
    "      \"| eos_id =\", tokenizer.eos_token_id)\n",
    "\n",
    "#  Label token sanity check\n",
    "true_ids  = tokenizer.encode(\"True\",  add_special_tokens=False)\n",
    "false_ids = tokenizer.encode(\"False\", add_special_tokens=False)\n",
    "\n",
    "print(f'\"True\" -> {true_ids} | \"False\" -> {false_ids}')\n",
    "\n",
    "if len(true_ids) != 1 or len(false_ids) != 1:\n",
    "    print(\"Warning: 'True' and/or 'False' are not single tokens. \"\n",
    "          \"We'll handle comparison by string prefix during evaluation.\")\n",
    "\n",
    "print(\"LoRA configured (r=32, Î±=64, dropout=0.05) with gradient checkpointing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "### **SFTTrainer Setup**\n",
    "\n",
    "Now we'll set up the `SFTTrainer` (Supervised Fine-tuning Trainer). This is the main tool from the `trl` library that will handle the entire training loop for us. We'll give it our model, tokenizer, dataset, and a set of training instructions, such as the batch size and number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 5: Start Training\\!**\n",
    "\n",
    "Now, we'll call the `train()` function on our `trainer` object. This will kick off the fine-tuning process.\n",
    "\n",
    "Grab a coffee, as this will take a few hours\\! â˜•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8823c6ad12984e3382b6c08826ee97e2",
      "bfbb502b1fb346b383758c184a7dd6a7",
      "b0cf0ac76c41448986150de584857319",
      "bb506434dfc2438095d0ce285dc079de",
      "bab47d1717824881922c0d2ae58a83d8",
      "3b8c16ab0d1c4e31971a7bc887d1cccb",
      "4b19fb9451c446158d33f2c6c3dc2450",
      "e8210c381ad14e5a8ad2380682fdef87",
      "0dadcfa53c404326b2cc0e8f1033548b",
      "6f5b6e7d2c7a4fdea9757f6726e7bf4f",
      "e115d34befa94d0f86024aee4dda14b1"
     ]
    },
    "id": "yVZHQ4y74BCG",
    "outputId": "fecdfd59-edc7-4d3d-fb21-8787511c66af"
   },
   "outputs": [],
   "source": [
    "# Step 4 (Version-safe): SFTTrainer configuration\n",
    "\n",
    "import math, torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "print(f\"bfloat16 supported: {BF16_OK}\")\n",
    "\n",
    "per_device_bs = 2\n",
    "grad_accum    = 8\n",
    "effective_bs  = per_device_bs * grad_accum\n",
    "\n",
    "approx_steps_per_epoch = math.ceil(len(formatted_train) / effective_bs)\n",
    "\n",
    "# IMPORTANT:\n",
    "# - If CAP_PER_CLASS = 20_000 (â‰ˆ40k train samples total), steps/epoch â‰ˆ 2500.\n",
    "#   We'll train ~3000 steps for ~1.2 epochs.\n",
    "#\n",
    "# - If CAP_PER_CLASS = 30_000 (â‰ˆ60k train samples total), steps/epoch â‰ˆ 3750.\n",
    "#   We'll train ~3800 steps for ~1.0 epoch.\n",
    "#\n",
    "# We'll pick max_steps dynamically based on actual dataset size so we converge\n",
    "# without severely undertraining or wildly overfitting.\n",
    "if approx_steps_per_epoch <= 3000:\n",
    "    target_max_steps = 3000   # ~1.2 epochs on ~40k samples\n",
    "else:\n",
    "    target_max_steps = 3800   # ~1 epoch on ~60k samples\n",
    "\n",
    "print(f\"Train size: {len(formatted_train)} | Val size: {len(formatted_val)}\")\n",
    "print(f\"Effective batch: {effective_bs} | ~steps/epoch: {approx_steps_per_epoch} | max_steps: {target_max_steps}\")\n",
    "\n",
    "\n",
    "#  Common kwargs for both modern and legacy TrainingArguments\n",
    "common_kwargs = dict(\n",
    "    per_device_train_batch_size = per_device_bs,\n",
    "    gradient_accumulation_steps = grad_accum,\n",
    "    warmup_ratio                = 0.05,\n",
    "    max_steps                   = target_max_steps,\n",
    "    learning_rate               = 1e-4,          # safer LR for LoRA r=32\n",
    "    weight_decay                = 0.01,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    fp16                        = (not BF16_OK),\n",
    "    bf16                        = BF16_OK,\n",
    "    logging_steps               = 20,\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = 200,\n",
    "    optim                       = \"adamw_8bit\",    # will use bitsandbytes if available\n",
    "    max_grad_norm               = 0.3,\n",
    "    seed                        = 42,\n",
    "    output_dir                  = \"outputs\",\n",
    "    report_to                   = \"none\",\n",
    "    remove_unused_columns       = False,\n",
    ")\n",
    "\n",
    "# Try modern args with evaluation; if not supported, fall back\n",
    "try:\n",
    "    train_args = TrainingArguments(\n",
    "        **common_kwargs,\n",
    "        evaluation_strategy      = \"steps\",\n",
    "        eval_steps               = 250,\n",
    "        load_best_model_at_end   = True,\n",
    "        metric_for_best_model    = \"eval_loss\",\n",
    "        greater_is_better        = False,\n",
    "    )\n",
    "    use_eval = True\n",
    "    print(\"Using TrainingArguments with evaluation_strategy='steps'.\")\n",
    "except TypeError:\n",
    "    train_args = TrainingArguments(**common_kwargs)\n",
    "    use_eval = False\n",
    "    print(\"Legacy TrainingArguments detected -> proceeding WITHOUT in-train evaluation.\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model               = model,\n",
    "    tokenizer           = tokenizer,\n",
    "    train_dataset       = formatted_train,\n",
    "    eval_dataset        = (formatted_val if use_eval else None),\n",
    "    dataset_text_field  = \"text\",\n",
    "    max_seq_length      = max_seq_length,\n",
    "    packing             = False,\n",
    "    args                = train_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# If we trained without eval (legacy fallback), at least save the last checkpoint.\n",
    "if not use_eval:\n",
    "    trainer.save_model(\"outputs-last\")\n",
    "    tokenizer.save_pretrained(\"outputs-last\")\n",
    "    print(\"Saved last checkpoint to outputs-last (no eval available).\")\n",
    "else:\n",
    "    # Trainer will have already restored best checkpoint by eval_loss\n",
    "    trainer.save_model(\"outputs-best\")\n",
    "    tokenizer.save_pretrained(\"outputs-best\")\n",
    "    print(\"âœ… Training finished â€” best checkpoint (by eval_loss) loaded and saved to outputs-best.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 6: Inference and Evaluation**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that our model is trained, we need to test it on our validation set. We'll use a slightly different prompt for inferenceâ€”one where we leave the `Output:` section blank for the model to complete.\n",
    "\n",
    "Let's test it on a single example from our validation set to see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agvQR_Ku5wWY",
    "outputId": "2b352692-6275-4ff5-a5bd-b706b277c42c"
   },
   "outputs": [],
   "source": [
    "# Step 6: Inference and Evaluation\n",
    "\n",
    "# We will:\n",
    "# 1. Reload the fine-tuned model from checkpoint.\n",
    "# 2. Compute log p(\"True\") vs log p(\"False\") for each validation example.\n",
    "# 3. Find the best threshold and report validation accuracy.\n",
    "# 4. Show one qualitative example.\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "# 0. Reload the trained model from checkpoint\n",
    "\n",
    "# NOTE:\n",
    "# - We saved \"outputs-last\" as our fine-tuned LoRA checkpoint.\n",
    "\n",
    "BF16_OK = is_bfloat16_supported()\n",
    "reload_dtype = torch.bfloat16 if BF16_OK else torch.float16\n",
    "\n",
    "max_seq_length = 2048  # match training config\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name        = \"outputs-last\",   # directory saved in Step 5\n",
    "    max_seq_length    = max_seq_length,\n",
    "    dtype             = reload_dtype,\n",
    "    load_in_4bit      = True,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "# safety: pad/eos consistency (must match what we enforced earlier)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# put model in eval/inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model reloaded on device: {device}\")\n",
    "\n",
    "\n",
    "# 1. Validation data reference\n",
    "\n",
    "# We assume `raw_val` from Step 3 is still in memory (the held-out, UNFORMATTED split\n",
    "# that still has question / solution / is_correct).\n",
    "# If it's not in memory in a fresh runtime, you must rebuild raw_val the same way as Step 3.\n",
    "validation_dataset = raw_val\n",
    "print(f\"Validation examples: {len(validation_dataset)}\")\n",
    "\n",
    "\n",
    "# 2. Prompt builder for inference (must match training style)\n",
    "\n",
    "# We trained with a deterministic instruction template that ends with the gold label.\n",
    "# For inference, we STOP right before the label and let the model choose.\n",
    "# We'll clip the solution text exactly like training.\n",
    "\n",
    "HEAD_TOK, TAIL_TOK = 800, 256  # must match Step 3 final values\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK):\n",
    "        return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "INFERENCE_TEMPLATE = (\n",
    "    \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "    \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "    \"otherwise reply 'False'.\\n\\n\"\n",
    "    \"Question:\\n{q}\\n\\n\"\n",
    "    \"Solution:\\n{s}\\n\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, solution: str) -> str:\n",
    "    q_str = \"\" if question is None else str(question)\n",
    "    s_str = smart_clip(\"\" if solution is None else str(solution))\n",
    "    return INFERENCE_TEMPLATE.format(q=q_str, s=s_str)\n",
    "\n",
    "\n",
    "# 3. Score a single prompt via next-token logprobs\n",
    "# We'll take the logits for the NEXT token after the prompt, then compare\n",
    "# log p(\"True\") vs log p(\"False\").\n",
    "# NOTE: We've already verified earlier that \"True\" and \"False\"\n",
    "# each map to a single token ID in this tokenizer.\n",
    "\n",
    "true_token_id  = tokenizer.encode(\"True\",  add_special_tokens=False)[0]\n",
    "false_token_id = tokenizer.encode(\"False\", add_special_tokens=False)[0]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_example(prompt: str):\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "    ).to(device)\n",
    "\n",
    "    out = model(**enc)\n",
    "    # logits shape: [1, seq_len, vocab]\n",
    "    last_logits = out.logits[0, -1]          # distribution for the NEXT token\n",
    "    logprobs = F.log_softmax(last_logits, dim=-1)\n",
    "\n",
    "    logp_true  = logprobs[true_token_id].item()\n",
    "    logp_false = logprobs[false_token_id].item()\n",
    "\n",
    "    margin = logp_true - logp_false          # >0 => model leans \"True\"\n",
    "    return margin, logp_true, logp_false\n",
    "\n",
    "\n",
    "# 4. Score entire validation set, tune threshold, compute accuracy\n",
    "\n",
    "margins = []\n",
    "golds = []\n",
    "\n",
    "for ex in tqdm(validation_dataset, desc=\"Scoring validation set\"):\n",
    "    q = ex[\"question\"]\n",
    "    s = ex[\"solution\"]\n",
    "    y = bool(ex[\"is_correct\"])  # gold label\n",
    "    prompt = build_prompt(q, s)\n",
    "    margin, _, _ = score_example(prompt)\n",
    "    margins.append(margin)\n",
    "    golds.append(y)\n",
    "\n",
    "margins = np.array(margins, dtype=np.float32)\n",
    "golds   = np.array(golds,   dtype=bool)\n",
    "\n",
    "# sweep candidate thresholds across the distribution of margins\n",
    "candidate_thresholds = np.quantile(margins, np.linspace(0.02, 0.98, 25))\n",
    "\n",
    "best_acc = -1.0\n",
    "best_th  = 0.0\n",
    "\n",
    "for th in candidate_thresholds:\n",
    "    preds = margins >= th     # predict True if margin >= threshold\n",
    "    acc = (preds == golds).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_th = th\n",
    "\n",
    "final_preds = margins >= best_th\n",
    "final_acc = (final_preds == golds).mean()\n",
    "\n",
    "true_rate_pred  = final_preds.mean()\n",
    "true_rate_gold  = golds.mean()\n",
    "\n",
    "print(\"=======================================\")\n",
    "print(f\"Validation size: {len(golds)}\")\n",
    "print(f\"Best threshold: {best_th:.6f}\")\n",
    "print(f\"Validation accuracy: {final_acc*100:.2f}%\")\n",
    "print(f\"Model 'True' rate after threshold: {true_rate_pred*100:.2f}%\")\n",
    "print(f\"Ground truth 'True' rate: {true_rate_gold*100:.2f}%\")\n",
    "print(\"=======================================\")\n",
    "\n",
    "# 5. Inspect one qualitative example\n",
    "\n",
    "idx = 10  # change to inspect other samples\n",
    "ex = validation_dataset[idx]\n",
    "q = ex[\"question\"]\n",
    "s = ex[\"solution\"]\n",
    "gold = bool(ex[\"is_correct\"])\n",
    "\n",
    "prompt = build_prompt(q, s)\n",
    "margin, lp_t, lp_f = score_example(prompt)\n",
    "pred_bool = (margin >= best_th)\n",
    "\n",
    "print(\"\\n#### QUESTION ####\")\n",
    "print(q)\n",
    "print(\"\\n#### SOLUTION ####\")\n",
    "print(s)\n",
    "print(\"\\n#### SCORES ####\")\n",
    "print(f\"log p('True')  = {lp_t:.6f}\")\n",
    "print(f\"log p('False') = {lp_f:.6f}\")\n",
    "print(f\"margin(True-False) = {margin:.6f}\")\n",
    "print(f\"threshold = {best_th:.6f}\")\n",
    "print(\"\\n#### MODEL PREDICTION ####\")\n",
    "print(pred_bool)\n",
    "print(\"\\n#### CORRECT ANSWER ####\")\n",
    "print(gold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 7: Generate Submission File**\n",
    "\n",
    "This is the final step\\! We will now run our fine-tuned model on the official `test` dataset.\n",
    "\n",
    "We will loop through each example in the test set, generate a prediction, and format the results into a CSV file with two columns: `ID` and `is_correct`, as required by the competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lvcDSh0JZYm",
    "outputId": "1ca1eee3-d69a-4aac-a2f4-5dee1dd4f914"
   },
   "outputs": [],
   "source": [
    "# Step 7: Generate Submission File (Test Inference â†’ CSV)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 0. Safety / setup\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Ensure we're in eval mode and on the right device\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Inference device: {device}\")\n",
    "\n",
    "# We MUST reuse the same inference template\n",
    "# that we used for validation thresholding in Step 6.\n",
    "# Keep wording identical so the logits are calibrated.\n",
    "INFERENCE_TEMPLATE = (\n",
    "    \"You are a rigorous mathematician. Determine if the provided solution correctly \"\n",
    "    \"solves the problem. Reply strictly with 'True' if it is fully correct, \"\n",
    "    \"otherwise reply 'False'.\\n\\n\"\n",
    "    \"Question:\\n{q}\\n\\n\"\n",
    "    \"Solution:\\n{s}\\n\\n\"\n",
    "    \"Answer:\\n\"\n",
    ")\n",
    "\n",
    "# Use the same clip lengths and max_seq_length that were used\n",
    "# for training/inference so distribution matches validation.\n",
    "HEAD_TOK, TAIL_TOK = 800, 256  # must match Step 6\n",
    "# max_seq_length should already be defined earlier (2048). We rely on that here.\n",
    "\n",
    "def smart_clip(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    toks = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(toks) <= (HEAD_TOK + TAIL_TOK):\n",
    "        return s\n",
    "    head = tokenizer.decode(toks[:HEAD_TOK], skip_special_tokens=True)\n",
    "    tail = tokenizer.decode(toks[-TAIL_TOK:], skip_special_tokens=True)\n",
    "    return head + \"\\n...\\n\" + tail\n",
    "\n",
    "def build_prompt(question: str, solution: str) -> str:\n",
    "    q_str = \"\" if question is None else str(question)\n",
    "    s_str = smart_clip(\"\" if solution is None else str(solution))\n",
    "    return INFERENCE_TEMPLATE.format(q=q_str, s=s_str)\n",
    "\n",
    "# Reuse the same label tokenization as Step 6.\n",
    "true_token_id  = tokenizer.encode(\"True\",  add_special_tokens=False)[0]\n",
    "false_token_id = tokenizer.encode(\"False\", add_special_tokens=False)[0]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def score_margin(prompt: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute margin = log p(True) - log p(False)\n",
    "    at the *next token* after the prompt.\n",
    "    Higher margin => lean 'True'.\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "    ).to(device)\n",
    "\n",
    "    out = model(**enc)\n",
    "    logits = out.logits[0, -1]          # logits for NEXT token\n",
    "    logprobs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    logp_true  = logprobs[true_token_id].item()\n",
    "    logp_false = logprobs[false_token_id].item()\n",
    "    return logp_true - logp_false       # decision score\n",
    "\n",
    "# 1. Load the official test split\n",
    "# -----------------------------------------------------\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"ad6398/nyu-dl-teach-maths-comp\",\n",
    "    split=\"test\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "print(f\"Test examples: {len(test_dataset)}\")\n",
    "\n",
    "# 2. Predict each test example with tuned threshold\n",
    "# -----------------------------------------------------\n",
    "# VERY IMPORTANT:\n",
    "# We use the SAME best_th found on validation in Step 6.\n",
    "# That threshold aligns model logits to True/False balance.\n",
    "# Do NOT recompute here (data leakage); just reuse best_th.\n",
    "\n",
    "predictions = []\n",
    "for ex in tqdm(test_dataset, desc=\"Scoring test\"):\n",
    "    q = ex[\"question\"]\n",
    "    s = ex[\"solution\"]\n",
    "    prompt = build_prompt(q, s)\n",
    "    margin = score_margin(prompt)\n",
    "    pred_bool = (margin >= best_th)\n",
    "    predictions.append(pred_bool)\n",
    "\n",
    "# 3. Build submission DataFrame\n",
    "# -----------------------------------------------------\n",
    "# Kaggle expects:\n",
    "#   ID          -> row index in test set\n",
    "#   is_correct  -> boolean (True/False)\n",
    "#\n",
    "# We map row index deterministically using enumerate to be explicit.\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": [i for i, _ in enumerate(predictions)],\n",
    "    \"is_correct\": predictions,\n",
    "})\n",
    "\n",
    "# 4. Save to CSV\n",
    "# -----------------------------------------------------\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved.\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5489c0ed"
   },
   "source": [
    "# SAVE THE MODEL TO DRIVE AND RUN INFERENCE\n",
    "Add code to save the model checkpoint to Google Drive, load the model from the checkpoint, and generate the final submission CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b1e0a43"
   },
   "source": [
    "## Mount google drive\n",
    "\n",
    "### Subtask:\n",
    "Mount Google Drive to save the model checkpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b8ab404"
   },
   "source": [
    "**Reasoning**:\n",
    "Mount Google Drive to save the model checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e020e6b",
    "outputId": "9111f788-23ea-40e4-9dc7-79f52777d9ba"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c28a7dd"
   },
   "source": [
    "## Save model checkpoint\n",
    "\n",
    "### Subtask:\n",
    "Save the trained model checkpoint to the specified path in Google Drive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe96ff59"
   },
   "source": [
    "**Reasoning**:\n",
    "Define the save path and save the model and tokenizer to Google Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ec9d6bf",
    "outputId": "2a0adc1e-7861-4178-f277-c63d1e5a3d01"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to save the model checkpoint in Google Drive\n",
    "save_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model checkpoint and tokenizer saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feb0b9a5"
   },
   "source": [
    "## Load model from checkpoint\n",
    "\n",
    "### Subtask:\n",
    "Load the model from the saved checkpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d984f7ec"
   },
   "source": [
    "**Reasoning**:\n",
    "Load the model and tokenizer from the saved checkpoint path in Google Drive and prepare the model for inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242,
     "referenced_widgets": [
      "858319004de8422a82b82b795c580e0b",
      "a186cdad6f05417a810ab1f33e216730",
      "eacc964e36f2452d9c316b16a14db6ff",
      "83bb9ddae35e4bd4a03cfb2361869c47",
      "ca95f7889587481c8b45508a6eee3e88",
      "133548848bff4725a65d60bc0fc647a8",
      "22d1ceea2ffc4827bd04d077a6761e26",
      "1987307094794c24b9e45a7401a6480f",
      "16e48c7c4fa242f49d9cf30c6468ca72",
      "b17162405f9d404fbbdd016d35d8d426",
      "4fd8baa5b5f64296bc4c091a389b72ff",
      "a7b4191a96a54b448c686dd0a211e182",
      "e05b444e196f4a41ad16183f502d0044",
      "1cba160aea124cefb1bb3134b7284b8c",
      "5f37fa6e464448f0b43ae83eab3ab422",
      "94e5f41be88743ba96e1c723becc8c3e",
      "833d915f367c43c3acf08a090ce74e20",
      "2bd0b577b3194cf38c28c87fc0eb7bd4",
      "216514ca61bf4f27851cf23690697b96",
      "ecd564788ca34242bae8dfc4d4252804",
      "a237664722bb47cabb0219e34b7686ef",
      "77742207a7184e9983ceadc55890f194"
     ]
    },
    "id": "cc269188",
    "outputId": "28cf1897-3530-48e7-e6e5-69f671b72976"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype          = dtype,\n",
    "    load_in_4bit   = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"âœ… Model and tokenizer successfully loaded from checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32f497a5"
   },
   "source": [
    "## Generate submission file\n",
    "\n",
    "### Subtask:\n",
    "Generate the submission CSV file using the loaded model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bc4e9ea"
   },
   "source": [
    "**Reasoning**:\n",
    "Generate the submission CSV file by iterating through the test dataset, generating predictions using the loaded model, and saving the results to a pandas DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "185bd13d",
    "outputId": "e0ec3ea4-e769-4d6c-fc86-42843a08e1b7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "predictions = []\n",
    "\n",
    "# Create the prompt template for inference (no answer included)\n",
    "inference_prompt = \"\"\"You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'True' if the solution is correct, otherwise 'False'. Below is the Question and Solution.\n",
    "Question:\n",
    "{}\n",
    "Solution:\n",
    "{}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# A simple function to parse 'True' or 'False' from the model's raw output\n",
    "def parse_output(response_text):\n",
    "    # Find the text after \"Output:\"\n",
    "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
    "    # Check if \"True\" is in that part, case-insensitively\n",
    "    if 'true' in output_part.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Loop through the test dataset and generate a prediction for each example\n",
    "for example in tqdm(test_dataset):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = inference_prompt.format(question, str(solution))\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the prediction\n",
    "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "    response_text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # Parse the prediction and add it to our list\n",
    "    prediction = parse_output(response_text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
    "print(\"You can now download this file and submit it to the Kaggle competition.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}